product = 'VX'
new_accounts_data = pd.read_csv(DOWNLOADS_DIR + f'{USE_DATE}_{product}_new_accounts.csv',
                                parse_dates=['Trade Date'])
daily_volume = new_accounts_data.groupby('Trade Date')['Size'].sum()/2
daily_volume.name = 'Volume'

# Aggregate to monthly ADV
yearmonth_strs = daily_volumes.index.strftime('%Y-%m')  # String form needed for .loc[]
monthly_adv = pd.Series()
for yearmonth_str in yearmonth_strs:
    monthly_adv.loc[pd.Timestamp(yearmonth_str)] = daily_volumes.loc[yearmonth_str].mean()
monthly_adv_percentiles = monthly_adv.rank(pct=True)    # Percentile over full history
monthly_adv_rolling_percentiles = \
    monthly_adv.rolling(12).apply(lambda x_ser: x_ser.rank(pct=True)[-1], raw=False)    # Percentile rolling 1-year
monthly_adv_df = pd.DataFrame({'ADV': monthly_adv, 'Percentile (Since 2018-03)': monthly_adv_percentiles,
                               'Percentile (1 Year)': monthly_adv_rolling_percentiles})

# IBIG
# 1) Load
ibig = (pd.read_csv(DOWNLOADS_DIR + f'IBIG_Daily_Volume_data_{USE_DATE}.csv',
                    parse_dates=['Month, Day, Year of Trading Dt'])
        .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
ibig_new_accounts = evaluate_new_accounts(vxm)
vxm_new_accounts.to_csv(DOWNLOADS_DIR + f'vxm_new_accounts_{USE_DATE}.csv')     # Export!
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
vxm_new_accounts_CTI = vxm_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
vxm_new_accounts_cumsum = vxm_new_accounts_CTI.cumsum()

# IBIG
# 1) Load
ibig = (pd.read_csv(DOWNLOADS_DIR + 'IBIG_Daily_Volume_data_2021-03-02.csv',
                    parse_dates=['Month, Day, Year of Trading Dt'])
        .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
ibig_new_accounts = evaluate_new_accounts(ibig)
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
ibig_new_accounts_CTI = ibig_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
ibig_new_accounts_cumsum = ibig_new_accounts_CTI.cumsum()
# 4) Export
ibig_new_accounts.to_csv(DOWNLOADS_DIR + 'ibig_new_accounts_2021-03-02.csv')

# Try it on IBHY
# 1) Load
ibhy = (pd.read_csv(DOWNLOADS_DIR + 'IBHY_Daily_Volume_data_2021-03-02.csv',
                    parse_dates=['Month, Day, Year of Trading Dt'])
        .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
ibhy_new_accounts = evaluate_new_accounts(ibhy)
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
ibhy_new_accounts_CTI = ibhy_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
ibhy_new_accounts_cumsum = ibhy_new_accounts_CTI.cumsum()
# 4) Export
ibhy_new_accounts.to_csv(DOWNLOADS_DIR + 'ibhy_new_accounts_2021-03-02.csv')

# Try it on big VIX
# 1) Load
vx = (pd.read_csv(DOWNLOADS_DIR + 'VX_Daily_Volume_data_2021-01-29.csv',
                  parse_dates=['Month, Day, Year of Trading Dt'])
      .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
vx_new_accounts = evaluate_new_accounts(vx)
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
vx_new_accounts_CTI = vx_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
vx_new_accounts_cumsum = vx_new_accounts_CTI.cumsum()


# def evaluate_volumes(data):
#     # Step 0: Check if any accounts switch CTI
#     ctiswitch_set = set(data[data['CTI'] == 4]['Account ']) & set(data[data['CTI'] != 4]['Account '])
#     if len(ctiswitch_set) != 0:
#         print(f"Accounts that switch CTI: {ctiswitch_set}")
#
#     # Step 1: Aggregate size by date, CTI, firm, account; each row will be unique
#     data_agg = data.groupby(['Trade Date', 'CTI', 'Name', 'Account '])['Size'].sum()
#
#     # Step 2: Generate "known" set of accounts for each day
#     data_days = data_agg.index.get_level_values('Trade Date').unique()
#     known_set_dict = {data_days[0]: set()}  # No known accounts on first day
#     for day, prev_day in zip(data_days[1:], data_days[:-1]):
#         known_set_dict[day] = \
#             (known_set_dict[prev_day]
#              | set(data_agg.loc[prev_day].index.get_level_values('Account ')))
#
#     # Step 3: Mark accounts each day that were not known at the beginning of the day
#     data_agg_reset = data_agg.reset_index('Account ')
#     for day in data_days:
#         # .values is great for doing stuff with no unique indexes
#         data_agg_reset.loc[day, 'New Account'] = \
#             (~data_agg_reset.loc[day]['Account '].isin(known_set_dict[day])).values
#     return data_agg_reset


# for col in ['Trade Time', 'Trade Date', 'Expire Date']:
#     modern_data[col] = pd.to_datetime(modern_data[col])     # Parse dates after concat to save time
# modern_data = modern_data.set_index('Trade Time').sort_index()

# axr.plot(trade_date_expiry_data['Size'], color='C1', label='Size')

# nine_three = vx_315.loc['2020-09-03']
# nine_three_exp1 = nine_three[nine_three['Expire Date'] == pd.Timestamp('2020-10-21')]

# Column sets
legacy_vwap_cols =

# Settlement for all of legacy is 3:15pm, so 30-second VWAP would be 3:14:30pm to 3:15:00pm
vwap_legacy = legacy_vix.between_time('15:14:30', '15:15')

# Settlement for modern requires incorporating mini-VIX on 2020-08-10 and switching to 2:59:30pm-3:00:00pm on 2020-10-26
vwap_modern_pre_mini = modern_vix.between_time('15:14:30', '15:15')


vwap_modern_mini =


# modern_data = pd.concat(modern_data_list).sort_values(['Trade Date', 'Trade Time', 'Expire Date'])[modern_volume_cols]
# legacy_data = pd.concat(legacy_data_list).sort_values(['Date', 'Entry Time', 'Expiry Date'])[legacy_volume_cols]

test = pd.read_csv(modern_data_dir+'20180320 VX Contra Trade Data.csv',
                   parse_dates=['Trade Date', 'Trade Time', 'Expire Date'])

# Fix Virtu switch
old_virtu = adv_change_data_indexed.loc['Virtu Financial BD LLC']
new_virtu = adv_change_data_indexed.loc['Virtu Americas LLC']
virtu_adv = old_virtu['ADV'].replace(np.NaN, 0) + new_virtu['ADV'].replace(np.NaN, 0)
virtu_diff = virtu_adv.diff()
adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'ADV'] = virtu_adv
adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'Difference in ADV'] = virtu_diff
adv_change_data_indexed = adv_change_data_indexed.drop(['Virtu Financial BD LLC', 'Virtu Americas LLC'])

adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'ADV'] = virtu_adv
adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'Difference in ADV'] = virtu_diff

{obj.maturity_date: self.maturity_date,
                                              obj.price: self.price,
                                              obj.trade_date: self.trade_date,
                                              obj.volume: self.volume,
                                              obj.open_interest: self.open_interest,
                                              obj.t_to_mat: self.t_to_mat,
                                              obj.rate: self.rate}

class CashInstr(Instrument):
    """
    Cash instrument, derived from financial instrument
    Defined by: 1) extracting a price time-series from Instrument through column names
    """
    def __init__(self, ts_df, **super_kwargs):
        """
        :param ts_df: time-series DataFrame with time and value
        :param super_kwargs: kwargs for passing to superclass
        """
        super().__init__(ts_df, **super_kwargs)

:param tradestats: (optional) relevant trade statistics, etc. to be included; default empty DataFrame

# # TYVIX Vol Regimes
# test = pd.read_csv('Y:/Research/Research1/Gaibo/S&P Webinar Figures/3 - predictive signaling raw data.csv', index_col='Date', parse_dates=True)
# _, axleft = plt.subplots()
# make_lineplot(tyvix.price(), 'TYVIX', ax=axleft)
# make_regime(tyvix.vol_regime()[2], 'High Vol Regime', 'grey', 'Date', 'Index Level', 'TYVIX Vol Regimes', ax=axleft)
# make_regime(tyvix.vol_regime()[1], 'Low Vol Regime', 'white', 'Date', 'Index Level', 'TYVIX Vol Regimes', ax=axleft)
# # axleft.autoscale(enable=True, axis='x', tight=True)
# axleft.legend(loc=2, fontsize=13)
# axleft.set_ylabel('Volatility Index (%)', fontsize=16)
# axright = axleft.twinx()
# axright.plot(test['10YR - 2YR Yield'], label='10yr - 2yr Yield', color='C1')
# axright.legend(loc=1, fontsize=13)
# axright.set_ylabel('% (Annualized)', fontsize=16)
# axright.set_xlim('2016-01-01', '2019-09-03')
# # axleft.autoscale(enable=True, axis='x', tight=True)
# # axright.set_ylim(20, 115)
# axleft.set_title('TYVIX Vol Regimes', fontsize=16)
# axleft.set_xlabel('Date', fontsize=16)

creditvix_data = pd.read_csv('data/creditvix_pc_bp_missing_4_months.csv',
                             index_col='Date', parse_dates=True)

tuvix = pd.read_csv('data/tuvix_cme.csv', index_col='Date', parse_dates=True)
fvvix = pd.read_csv('data/fvvix_cme.csv', index_col='Date', parse_dates=True)
tyvix_cme = pd.read_csv('data/tyvix_cme.csv', index_col='Date', parse_dates=True)
usvix = pd.read_csv('data/usvix_cme.csv', index_col='Date', parse_dates=True)

ten_two_list_low = get_regime_data_list(tyvix_lows, test['10YR - 2YR Yield'])
combined_ten_two_low = combine_data_list(map(lambda df: df.pct_change(), ten_two_list_low))
ten_two_list_high = get_regime_data_list(tyvix_highs, test['10YR - 2YR Yield'])
combined_ten_two_high = combine_data_list(map(lambda df: df.pct_change(), ten_two_list_high))

# regime_10_2 = pd.DataFrame({'regime': tyvix_hl, '10-2': test['10YR - 2YR Yield']}).dropna()
# # Lows
# low_acc = pd.Series()
# for interval in tyvix_lows:
#     changes = regime_10_2['10-2'].loc[interval[0]:interval[1]].diff().dropna()
#     low_acc = low_acc.append(changes)
# low_acc.mean()
# # Highs
# high_acc = pd.Series()
# for interval in tyvix_highs:
#     changes = regime_10_2['10-2'].loc[interval[0]:interval[1]].diff().dropna()
#     high_acc = high_acc.append(changes)
# high_acc.mean()

cdx_ig = Index(bbg_data['IBOXUMAE CBBT Curncy', 'PX_LAST'], 'CDX NA IG')
itraxx_ie = Index(bbg_data['ITRXEBE CBBT Curncy', 'PX_LAST'], 'iTraxx EU Main')
itraxx_xo = Index(bbg_data['ITRXEXE CBBT Curncy', 'PX_LAST'], 'iTraxx EU Xover')
itraxx_fs = Index(bbg_data['ITRXESE CBBT Curncy', 'PX_LAST'], 'iTraxx EU SenFin')

scaled_cdx_ig = cdx_ig.loc[roll_dates_df['CDX IG'].iloc[-1]:]   # Start with just the current series

srvix_undl_rv = (srvix.underlying.price().rolling(252).apply(
                     lambda p_252: (p_252.diff()**2).mean()**0.5 * 252**0.5, raw=False)) * 100
srvix_undl_rv = srvix_undl_rv.shift(-252)

full_data = pd.read_csv('../data/price_index_data.csv', index_col='Date', parse_dates=True)
eurostoxx_data = pd.read_csv('../data/sx5e_data.csv', index_col='Date', parse_dates=True)
sptr_data = pd.read_csv('../data/sptr_vix_data.csv', index_col='Date', parse_dates=True)
agg_data = pd.read_csv('../data/agg_data.csv', index_col='Date', parse_dates=True)

import sys
sys.path.insert(0, '/Users/gaibo.zhang/PycharmProjects/analysis_package/utility/')

[truncd_spx, truncd_ty1] = share_dateindex([spx.price(), ty1.price()])
make_lineplot([truncd_spx/truncd_spx[0], truncd_ty1/truncd_ty1[0]],
              ['SPX cumulative return', 'TY1 cumulative return'],
              ['C0', 'C4'])

[truncd_spx, truncd_ty1] = share_dateindex([spx.price(), ty1.price()])
make_lineplot([truncd_spx/truncd_spx[0], truncd_ty1/truncd_ty1[0]],
              ['SPX cumulative return', 'TY1 cumulative return'])

[joined_x_data, joined_y_data] = share_dateindex([instr_x.price_return(),
                                                                  instr_y.price_return()])
                x = joined_x_data.values.reshape(-1, 1)
                y = joined_y_data.values
                model = LinearRegression(fit_intercept=False).fit(x, y)

color_dict = {frozenset({spx, vix}): 'C1', frozenset({spx, hyg}): 'C2', frozenset({spx, ief}): 'C3',
                  frozenset({spx, sx5e}): 'C4', frozenset({spx, agg}): 'C5',
                  frozenset({vix, hyg}): 'C1', frozenset({vix, ief}): 'C2', frozenset({vix, sx5e}): 'C3',
                  frozenset({vix, agg}): 'C4',
                  frozenset({hyg, ief}): 'C1', frozenset({hyg, sx5e}): 'C2', frozenset({hyg, agg}): 'C3',
                  frozenset({ief, sx5e}): 'C1', frozenset({ief, agg}): 'C2',
                  frozenset({sx5e, agg}): 'C1'}

calc_log_returns = lambda arr: np.log(arr[-1]/arr[0]) if len(arr)>1 else np.NaN

elif granularity == 'intraday':
if time_start != time_end:
    print("WARNING: only time_start parameter is used for intraday.")
intraday_day = truncd_levels.loc[time_start, time_start + ONE_DAY]
return intraday_day.resample(intraday_interval, label='right', closed='right').pad()
elif granularity == 'multiday':
# NOTE: still uncertain about how to do this
return truncd_levels.resample(multiday_interval).pad()


def log_returns(self, time_start=None, time_end=None,
                do_resample=False, resample_interval=pd.Timedelta(minutes=5)):
    """ Calculate log returns
    :param time_start: start of time-series to use
    :param time_end: end of time-series to use
    :param do_resample: set True to resample to the granularity of <resample_interval>
    :param resample_interval: only relevant if resampling
    :return: pd.Series with 'time' and 'value'
    """
    if time_start is None:
        time_start = self.levels.first_valid_index()
    if time_end is None:
        time_end = self.levels.last_valid_index()
    truncd_levels = self.levels.truncate(time_start, time_end)
    if not self.is_intraday or not do_resample:
        # Data is already regular (i.e. daily or fixed-interval intraday) -
        # avoid messing with business days, etc.
        return np.log(truncd_levels).diff()
    else:
        # Data is intraday and irregular - resample to get fixed intervals
        resampled_truncd_levels = \
            truncd_levels.resample(resample_interval,
                                   label='right', closed='right').pad()
        return np.log(resampled_truncd_levels).diff()

table = pd.concat([vix_prices.describe().iloc[0:1].astype(int),
                       vix_prices.describe().iloc[1:],
                       vix_pct_changes.describe().iloc[1:]*100])

def make_scatterplot(x_data, y_data, xlabel=None, ylabel=None, title=None, ax=None):
    # Prepare Figure and Axes
    if ax is None:
        fig, ax = plt.subplots()
    else:
        fig = None
    # Plot
    x_data.to_frame(x_data.name).join(y_data.to_frame(y_data.name))
    ax.scatter(x_data, y_data)
    ax.grid()
    ax.plot([0, 1], [0, x_data.corr(y_data)], 'k')
    # Set labels
    if xlabel:
        ax.set_xlabel(xlabel)
    if ylabel:
        ax.set_ylabel(ylabel)
    if title:
        ax.set_title(title)
    return fig, ax

def make_scatterplot(x_instr, y_instr, ax=None):
    # Prepare Figure and Axes
    if ax is None:
        fig, ax = plt.subplots()
    else:
        fig = None
    # Plot
    x_returns = x_instr.price_return(logarithmic=False)
    y_returns = y_instr.price_return(logarithmic=False)
    joined_data = x_returns.to_frame(x_instr.name).join(y_returns.to_frame(y_instr.name), how='inner')
    ax.scatter(joined_data[x_instr.name], joined_data[y_instr.name])
    ax.grid()
    corr_x_y = x_returns.corr(y_returns)
    x_min = x_returns.min()
    x_max = x_returns.max()
    ax.plot([x_min, x_max], [x_min*corr_x_y, x_max*corr_x_y], 'k')
    # Set labels
    ax.set_xlabel("{} % Change".format(x_instr.name))
    ax.set_ylabel("{} % Change".format(y_instr.name))
    ax.set_title("Daily Percent Change: {} vs {}".format(x_instr.name, y_instr.name))
    return fig, ax

ax.set_xlabel("{} % Change".format(x_instr.name))
    ax.set_ylabel("{} % Change".format(y_instr.name))
    ax.set_title("Daily Percent Change: {} vs {}".format(x_instr.name, y_instr.name))

joined_data = x_data.to_frame('x').join(y_data.to_frame('y'), how='inner').dropna()

# Execute organized commands to create desired analyses
