        try:
            out_of_stock = [float(s.attrs['value'].split(':')[-1]) for s in disabled_sizes]     # Try convert '9' to 9
            in_stock = [float(s.attrs['value'].split(':')[-1]) for s in enabled_sizes]
            if COLLECTION_DATAFRAME.empty:
                # Re-initialize column order with sorted numeric sizes
                COLLECTION_DATAFRAME = pd.DataFrame(columns=sorted(out_of_stock + in_stock))
        except ValueError:
            out_of_stock = [s.attrs['value'].split(':')[-1] for s in disabled_sizes]    # Keep 'M' as string
            in_stock = [s.attrs['value'].split(':')[-1] for s in enabled_sizes]
            if COLLECTION_DATAFRAME.empty:
                # Re-initialize hard-coded column order to non-numeric sizes (may include non-existent sizes)
                COLLECTION_DATAFRAME = pd.DataFrame(columns=['XS', 'S', 'M', 'L', 'XL', '2XL', '3XL'])

        select_sizes_section_search = nike_response.html.find("div.mt2-sm.css-hzulvp")
        select_sizes_section = select_sizes_section_search[0]
        disabled_sizes = select_sizes_section.find(":disabled")
        enabled_sizes = select_sizes_section.find(":enabled")

nike_response = requests.get(NIKE_LINK)
nike_soup = BeautifulSoup(nike_response.text, features='lxml')

        with open(readable_param_patch_msg_name[0], 'rb') as msg_name:
            msg_name_data = xmltodict.parse(f)

def collect_keylist_valuelist(nested_dict : dict,
                              keylist: List = None, valuelist: List = None) -> Tuple[List[Any], List[Any]]:
    """ Return list of all nested dictionary keys and list of all dictionary values after walking it
        NOTE: this is meant to parallel "walking" file structure directories; see collect_dirlist_filelist()
        NOTE: while collect_dirlist_filelist() does not need to worry about
    :param nested_dict: Python dictionary
    :param keylist: used to pass state during recursion; set None if using with root (outermost) dict!
    :param valuelist: used to pass state during recursion; set None if using with root (outermost) dict!
    :return: (list of keys, list of values)
    """
    # Wait, we need to worry about duplicate keys; outputted list with directories because no duplicate
    if keylist is None:
        keylist = []
    if valuelist is None:
        valuelist = []
    for key, value in nested_dict.items():
        keylist.append(key)


# And we could brute force and search for other stuff cycling through the other slots lmao
    # Alternatively, we actually look for each folder in our "known" list
    # Print (aesthetically) the discovered list
    # Verify that source slot number is in each; print otherwise
    # Should be able to pipe to debug: result should raise error or None on failing to detect slot (can't even start),
    # otherwise return True for no issues, False for issues, either way print messages (we can redirect prints)

    # We're gonna do a DFS basically; recursive, so each step needs to be progress the work
    # Rules of printing:
    #   - assume you are a directory and your name is already printed (root node will be done by wrapper)
    #   - sort your children - directories first, files second, alphabetical (maybe by suffix??)
    #   - print children by sorted order, recursing on directories

mod_name_slot_match_context = mod_name_slot_first_match.group(0)  # e.g. 'c03' or 'C06'

# This is a Frankenstein code example (from StackOverflow) for using ABCs to do generic types
# It is easily much clearer to do this with Protocols rather than ABCs!
# Though it goes from "TypeError instantiating an abstract method on creation" to "static type checker error"
class Comparable(metaclass=ABCMeta):
    @abstractmethod
    def __lt__(self, other: Any) -> bool: ...
CT = TypeVar('CT', bound=Comparable)    # Generic type where comparisons are possible! (All you need is <)

# Complicated cases describing how someone might use a ratio split in relation to "center" of range
    split_size_floor = int(split * (right_bound - left_bound))  # Account for possibility of non-int via 2 pivots later
    if ((split_insideoutside == 'inside' and prev_leftright == 'right')
            or (split_insideoutside == 'outside' and prev_leftright == 'left')):
        # Put split from left to right - e.g. split=0.7 -> [-------|---]
        pivot_left = left_bound + split_size_floor
        pivot_right = pivot_left + 1    # Next int
    elif ((split_insideoutside == 'inside' and prev_leftright == 'left')
            or (split_insideoutside == 'outside' and prev_leftright == 'right')):
        # Put split from right to left - e.g. split=0.7 -> [---|-------]
        pivot_right = right_bound - split_size_floor
        pivot_left = pivot_right - 1    # Previous int
    else:
        raise ValueError(f"IMPOSSIBLE: ratio split case ('{split_insideoutside}', '{prev_leftright}')")

    # Split towards number's appropriate bounds
    if verbose:
        print(f"Evaluation {completed_sections+1}:")
        print(f"[{left_bound}, {pivot_left}] | [{pivot_right}, {right_bound}]")
    if number <= pivot_left:
        if verbose:
            print(f"Went LEFT!")
        return _section_search(number, left_bound, pivot_left, split, split_insideoutside,
                               'left', completed_sections+1, verbose)
    elif number >= pivot_right:
        if verbose:
            print(f"Went RIGHT!")
        return _section_search(number, pivot_right, right_bound, split, split_insideoutside,
                               'right', completed_sections+1, verbose)
    else:
        raise ValueError(f"IMPOSSIBLE: number ({number}) could not split using "
                         f"the 2 integer pivots: ...{pivot_left}][{pivot_right}...")


        else:
            raise ValueError(f"split_insideoutside must be 'inside' or 'outside', not '{split_insideoutside}'")
    if prev_leftright == 'right':
        if split_insideoutside == 'inside':
            # Put split from left to right
            pivot_left = left_bound + split_floor_int
            pivot_right = pivot_left + 1
        if split_insideoutside == 'outside':
            # Put split from right to left
            pivot_right = right_bound - split_floor_int
            pivot_left = pivot_right - 1
        else:
            raise ValueError(f"split_insideoutside must be 'inside' or 'outside', not '{split_insideoutside}'")
    else:
        raise ValueError(f"prev_leftright must be 'left' or 'right', not '{prev_leftright}'")

            # TODO: oooh I think this is actually wrong - it's not variance (diff from mean), it's diff from 0
            # result = np.sqrt(self.price_return().rolling(window).var(ddof=0)
            #                  * BUS_DAYS_IN_YEAR)

foo = pd.read_csv('C:/Users/gaibo/PycharmProjects/gzap/data/vix_ohlc.csv', index_col='Date', parse_dates=True)
foo = foo.sort_index().dropna()
vix_test = foo['VIX Close']
from model.data_structures import CashInstr
vix = CashInstr(vix_test, name='VIX')

from models.data_structures import Instrument
burr = pd.DataFrame([[1,2,3,],[4,5,6],[7,8,9]])
test = Instrument(burr, 'burr')

        try:
            instance.__dict__['loc'] = instance.__dict__[self._name].loc
            setattr(instance, 'loc', getattr(getattr(instance, self._name), 'loc'))     # Ooh that is ugly
        except AttributeError:
            if hasattr(instance, 'loc'):
                delattr(instance, 'loc')    # Preserve AttributeError if new input doesn't have .loc

@property
def raw_data_df(self):
    """ Property-based managed attribute - getter """
    return self._raw_data_df

@raw_data_df.setter
def raw_data_df(self, value):
    """ Property-based managed attribute - setter """
    if isinstance(value, pd.DataFrame) or isinstance(value, pd.Series):
        self._raw_data_df = value.copy()    # Prevent DataFrame state confusion by deep copying
    else:
        print(f"WARNING: Instrument input data is not DataFrame/Series - things may not work "
              f"until you re-set the 'raw_data_df' attribute:\n{value}")
        self._raw_data_df = value   # None passes here; I'm leaving in flexibility to change input later
    self.loc = getattr(value, 'loc', None)  # Allow native use of DataFrame/Series .loc[] indexing (if available)

one_node_layer = tf.keras.layers.Dense(units=1, input_shape=(1,), name='Output')

    # Specifically gather model's root mean squared error at each epoch
    rmse_training = history_df["root_mean_squared_error"]
    rmse_validation = history_df["val_root_mean_squared_error"]

# my_feature = "median_income"    # Median income on a specific city block
    # my_label = "median_house_value"     # Median value of a house on a specific city block

DOLLAR_SCALE_FACTOR = 1000.0    # Scale house value label to be more readable/usable
    train_df["median_house_value"] /= DOLLAR_SCALE_FACTOR
    test_df["median_house_value"] /= DOLLAR_SCALE_FACTOR

# product_orderfill = product_data[ORDERFILL_FIELDS]
    # product_settleoi = product_data[SETTLEOI_FIELDS]
    # product_constructed = product_data[CONSTRUCTED_FIELDS]
    # # Crop NaNs from legacy data clash
    # modern_start = product_orderfill['Volume'].first_valid_index()[0]   # Volume field used as representative
    # product_orderfill = product_orderfill.loc[modern_start:]
    # product_constructed = product_constructed.loc[modern_start:]    # Constructed field limited by Order Fill history
    #
    # # Initialize storage dictionary
    # percentile_dict = {}
    #
    # # Run percentiles on each volume-based field from Order Fill
    # for i_field in ORDERFILL_FIELDS:
    #     percentile_dict[i_field] = generate_volume_percentiles(product_orderfill, i_field)
    #
    # # Run percentiles on OI field from Settlement+OI. OI's aggregation is very specific: it must be summed up to
    # # daily level (since the roll drives OI at expiry-level), then averaged up to monthly/quarterly/yearly
    # percentile_dict['OI'] = generate_volume_percentiles(product_settleoi, 'OI')
    # for i_agg in ['Monthly', 'Quarterly', 'Yearly']:
    #     percentile_dict['OI'][i_agg]['Sum'] = None    # This field makes no sense for OI
    #
    # # Run percentiles on constructed Notional field
    # percentile_dict['Notional'] = generate_volume_percentiles(product_constructed, 'Notional')
    #
    # # Store in higher level storage dictionary
    # product_dict[product] = percentile_dict


# DASHBOARD_DOWNLOAD_FILE = f'Unified_Data_Table_data_{USE_DATE}_NoWeeklys.csv'
# DASHBOARD_DOWNLOAD_FILE = f'Unified_Data_Table_data_{USE_DATE}_OnlyWeeklys.csv'

settle_data['Weekly'] = settle_data['Symbol'].apply(lambda s: s[-5:-3].isnumeric())     # Test for week number (VX)

# iboxx_mat_df = pd.DataFrame({'iBoxx Maturity': iboxx_maturities})
# iboxx_mat_df['Prev iBoxx Maturity'] = iboxx_mat_df['iBoxx Maturity'].shift(1)
# iboxx_mat_df['Bus Days Since Last Maturity'] = \
#     iboxx_mat_df.loc[1:].apply(lambda row: days_between(row['Prev iBoxx Maturity'],
#                                                         row['iBoxx Maturity'], True), axis=1)
# iboxx_mat_df = iboxx_mat_df.dropna().reset_index(drop=True)
# day = pd.Timestamp('2021-08-02')
# day_iboxx_context_idx = iboxx_mat_df['iBoxx Maturity'].searchsorted(day, side='right')
# day_next_mat, day_prev_mat, n_busdays = iboxx_mat_df.loc[day_iboxx_context_idx]
# n_days_to_next_mat = days_between(day, day_next_mat, True)
# nth_day_of_frame = days_between(day_prev_mat, day, True) + 1

reg = LinearRegression(fit_intercept=False).fit(hyg_returns_crop.values.reshape(-1, 1),
                                                ibxxibhy_returns_crop.values)
returns_crop_df.rolling(126).apply(lambda segment: print(segment), raw=False)


AFX_PRICE_FORMAT_CHANGE = pd.Timestamp('2017-08-18')
afx_price_format_change_idxs_1 = (afx_data['Trade Date'] < AFX_PRICE_FORMAT_CHANGE)
afx_price_format_change_idxs_2 = (afx_data['Trade Date'] == AFX_PRICE_FORMAT_CHANGE) & (afx_data['price'] <= 1400)
afx_price_format_change_idxs_3 = (afx_data['Trade Date'] == AFX_PRICE_FORMAT_CHANGE) & (afx_data['price'] > 1400)
afx_price_format_change_idxs_4 = (afx_data['Trade Date'] > AFX_PRICE_FORMAT_CHANGE)
afx_data.loc[afx_price_format_change_idxs_1 | afx_price_format_change_idxs_2, 'Interest Rate'] = \
    afx_data.loc[afx_price_format_change_idxs_1 | afx_price_format_change_idxs_2, 'price'] / 1000   # Proper way to read
afx_data.loc[afx_price_format_change_idxs_3 | afx_price_format_change_idxs_4, 'Interest Rate'] = \
    afx_data.loc[afx_price_format_change_idxs_3 | afx_price_format_change_idxs_4, 'price'] / 1000 / 100

# Temp 2021 test
# test = combined_data.set_index('Date').loc['2021-01-04':'2021-03-19']
test = combined_data.set_index('Date').loc['2016-01-15':'2021-03-19']
test_dates = test.index.unique()
# test_daily_totals = test.groupby('Date')['DBPV'].sum()
# test['Daily Total DBPV'] = test_daily_totals
# test['Transaction DBPV Weight'] = test['DBPV'] / test['Daily Total DBPV']
# test['Transaction Weighted Interest Rate'] = test['Transaction DBPV Weight'] * test['Interest Rate']
VOLUME_THRESHOLD_NOT_METS = []
SAFEGUARD_NEEDEDS = []
TERM_30_RATES_FIRSTPASS = []
TERM_30_RATES = []
DAY_DF_DICT_FIRSTPASS = {}
DAY_DF_DICT = {}
for i, (five_days_ago, today) in enumerate(zip(test_dates[:-4], test_dates[4:])):
    print(today.strftime('%Y-%m-%d'))

    # Test set of 5 days for volume threshold, extending beyond 5 as needed to meet minimum
    test_curr_five_days = test.loc[five_days_ago:today].copy()
    curr_five_day_volume = test_curr_five_days['Principal Amount'].sum()
    if curr_five_day_volume < 25e9:
        failed_five_day_volume = curr_five_day_volume
        n_days_extended = 0
        extend_success = False
        while curr_five_day_volume < 25e9:
            n_days_extended += 1
            new_date_idx = i - n_days_extended
            if new_date_idx < 0:
                # Fail: ran out of dates to extend
                TERM_30_RATES.append((today, None))
                VOLUME_THRESHOLD_NOT_METS.append((today, failed_five_day_volume, None, None))
                break
            test_curr_five_days = test.loc[test_dates[new_date_idx]:today].copy()
            curr_five_day_volume = test_curr_five_days['Principal Amount'].sum()
        else:
            # Success: reaching here means volume was successfully extended
            extend_success = True
            VOLUME_THRESHOLD_NOT_METS.append((today, failed_five_day_volume, n_days_extended, curr_five_day_volume))
        if not extend_success:
            continue    # Just move on to next date, skipping process

    # Apply 250bp filter on DTCC data (does not apply to AFX data apparently)
    prev_rate = TERM_30_RATES[-1][1]
    test_curr_five_days_safeguard = \
        test_curr_five_days[~((test_curr_five_days['Product Type'] == 'CP')
                              | (test_curr_five_days['Product Type'] == 'CD'))
                            | ~((test_curr_five_days['Interest Rate'] > prev_rate + 2.5)
                                | (test_curr_five_days['Interest Rate'] < prev_rate - 2.5))].copy()
    DAY_DF_DICT_FIRSTPASS[today] = test_curr_five_days
    DAY_DF_DICT[today] = test_curr_five_days_safeguard

    # Re-sort for 5-day window
    test_curr_five_days = test_curr_five_days.sort_values(['DBPV', 'Principal Amount'], ascending=[False, False])
    # Calculate total DBPV
    test_curr_five_days['5-Day Total DBPV'] = test_curr_five_days['DBPV'].sum()
    # Calculate each transaction's DBPV weight
    test_curr_five_days['Transaction DBPV Weight'] = \
        test_curr_five_days['DBPV'] / test_curr_five_days['5-Day Total DBPV']
    # Calculate each transaction's weighted interest rate
    test_curr_five_days['Transaction Weighted Interest Rate'] = \
        test_curr_five_days['Transaction DBPV Weight'] * test_curr_five_days['Interest Rate']
    # Calculate AMERIBOR Term-30 rate - FIRST PASS
    term_30_rate_firstpass = test_curr_five_days['Transaction Weighted Interest Rate'].sum()
    # Safeguard - check for transactions outside of 250bp range from first pass rate
    test_curr_five_days_safeguard = \
        test_curr_five_days[~((test_curr_five_days['Interest Rate'] > term_30_rate_firstpass + 2.5)
                              | (test_curr_five_days['Interest Rate'] < term_30_rate_firstpass - 2.5))].copy()
    # Calculate SECOND PASS
    test_curr_five_days_safeguard['5-Day Total DBPV'] = test_curr_five_days_safeguard['DBPV'].sum()
    test_curr_five_days_safeguard['Transaction DBPV Weight'] = \
        test_curr_five_days_safeguard['DBPV'] / test_curr_five_days_safeguard['5-Day Total DBPV']
    test_curr_five_days_safeguard['Transaction Weighted Interest Rate'] = \
        test_curr_five_days_safeguard['Transaction DBPV Weight'] * test_curr_five_days_safeguard['Interest Rate']
    term_30_rate = test_curr_five_days_safeguard['Transaction Weighted Interest Rate'].sum()
    if term_30_rate_firstpass != term_30_rate:
        SAFEGUARD_NEEDEDS.append((today, test_curr_five_days.shape[0]-test_curr_five_days_safeguard.shape[0],
                                  term_30_rate_firstpass, term_30_rate))
    TERM_30_RATES_FIRSTPASS.append((today, term_30_rate_firstpass))
    TERM_30_RATES.append((today, term_30_rate))
    DAY_DF_DICT_FIRSTPASS[today] = test_curr_five_days
    DAY_DF_DICT[today] = test_curr_five_days_safeguard
test_rates_firstpass = (pd.DataFrame(TERM_30_RATES_FIRSTPASS, columns=['Date', 'Term-30 Rate (First Pass)'])
                        .set_index('Date'))
test_rates = pd.DataFrame(TERM_30_RATES, columns=['Date', 'Term-30 Rate']).set_index('Date')
threshold_info = (pd.DataFrame(VOLUME_THRESHOLD_NOT_METS, columns=['Date', 'Failed 5-Day Volume',
                                                                   'Days Extended', 'Extended Volume'])
                  .set_index('Date'))
safeguard_info = (pd.DataFrame(SAFEGUARD_NEEDEDS, columns=['Date', 'Transactions Omitted',
                                                           'First Pass Rate', 'Second Pass Rate'])
                  .set_index('Date'))


####

# product_orderfill_subsplits = product_orderfill.copy()
    # for i_field in ['Standard', 'TAS', 'Block', 'ECRP', 'Spreads']:
    #     product_orderfill_subsplits[i_field] = product_orderfill[i_field] / product_orderfill['Volume']
    #     percentile_dict[i_field + ' Percent'] = generate_volume_percentiles(product_orderfill_subsplits, i_field)
    #     for i_agg in ['Monthly', 'Quarterly', 'Yearly']:
    #         percentile_dict[i_field + ' Percent'][i_agg]['Sum'] = None  # This field makes no sense for percent splits


from cboe_exchange_holidays_v3 import get_cboe_holidays
uvxy_holidays = get_cboe_holidays(start_datelike=uvxy_start)
uvxy_test = uvxy[uvxy_start:].drop(uvxy_holidays, errors='ignore')

# field_percentile_dict['Daily'] = {}
    # field_percentile_dict['Daily']['Sum'] = daily_field
    # field_percentile_dict['Daily']['ADV'] = daily_field
    # field_percentile_dict['Daily']['Percentile (All)'] = daily_percentile
    # field_percentile_dict['Daily']['Percentile (Last 252)'] = daily_percentile_rolling
    # field_percentile_dict['Daily']['ADV Change'] = daily_change
    # field_percentile_dict['Daily']['ADV Change Percentile'] = daily_change_percentile
    # field_percentile_dict['Daily']['ADV Change Percentile Pos/Neg'] = daily_change_posneg_percentile_df


# (by sum) for percentiles to make any sense. Otherwise the roll obscures expiry-level OI change.
field = 'OI'
# Extract volume-based field up to daily level - aggregate volume by summing
daily_field = product_settleoi.groupby(['Date'])[field].sum()
# Attach month, quarter, and year to each row to allow groupby()
yearmonth_col = pd.to_datetime(daily_field.index.strftime('%Y-%m'))
yearquarter_col = \
    pd.to_datetime(daily_field.index.to_series()
                   .apply(lambda ts: f'{ts.year}-{month_to_quarter_shifter(ts.month):02}'))
year_col = pd.to_datetime(daily_field.index.strftime('%Y'))
daily_field_df = pd.DataFrame({field: daily_field, 'Month': yearmonth_col,
                               'Quarter': yearquarter_col, 'Year': year_col})
# No aggregation - daily
# NOTE: at daily level, "volume" and "ADV" are conceptually the same
daily_percentile = daily_field.rank(pct=True)    # Percentile over full history
daily_percentile_1_year = daily_field.rolling(256).apply(lookback_rank, raw=False)    # Percentile rolling 1-year
# Aggregate to monthly
# NOTE: this was originally written with field='Volume', and I've generalized it to work
#       with any field. "average daily volume" (ADV) is therefore too specific, but I won't
#       change that for fear of confusion. so think of it as "average daily value" instead.
# NOTE: ADV is what should be ranked - different months have different numbers of days, so sum doesn't work
monthly_field = daily_field_df.groupby('Month')[field].sum()
monthly_adv = daily_field_df.groupby('Month')[field].mean()
monthly_percentile = monthly_adv.rank(pct=True)
monthly_percentile_1_year = monthly_adv.rolling(12).apply(lookback_rank, raw=False)    # Percentile rolling 1-year
# Aggregate to quarterly
quarterly_field = daily_field_df.groupby('Quarter')[field].sum()
quarterly_adv = daily_field_df.groupby('Quarter')[field].mean()
quarterly_percentile = quarterly_adv.rank(pct=True)
quarterly_percentile_1_year = quarterly_adv.rolling(4).apply(lookback_rank, raw=False)  # Percentile rolling 1-year
# Aggregate to yearly
yearly_field = daily_field_df.groupby('Year')[field].sum()
yearly_adv = daily_field_df.groupby('Year')[field].mean()
yearly_percentile = yearly_adv.rank(pct=True)
yearly_percentile_2_year = yearly_adv.rolling(2).apply(lookback_rank, raw=False)  # Percentile rolling 2-year

# Store
field_percentile_dict[field] = {}
field_percentile_dict[field]['Daily'] = {}
field_percentile_dict[field]['Daily']['Sum'] = daily_field
field_percentile_dict[field]['Daily']['ADV'] = daily_field
field_percentile_dict[field]['Daily']['Percentile (Total)'] = daily_percentile
field_percentile_dict[field]['Daily']['Percentile (1-Year)'] = daily_percentile_1_year
field_percentile_dict[field]['Monthly'] = {}
field_percentile_dict[field]['Monthly']['Sum'] = monthly_field
field_percentile_dict[field]['Monthly']['ADV'] = monthly_adv
field_percentile_dict[field]['Monthly']['Percentile (Total)'] = monthly_percentile
field_percentile_dict[field]['Monthly']['Percentile (1-Year)'] = monthly_percentile_1_year
field_percentile_dict[field]['Quarterly'] = {}
field_percentile_dict[field]['Quarterly']['Sum'] = quarterly_field
field_percentile_dict[field]['Quarterly']['ADV'] = quarterly_adv
field_percentile_dict[field]['Quarterly']['Percentile (Total)'] = quarterly_percentile
field_percentile_dict[field]['Quarterly']['Percentile (1-Year)'] = quarterly_percentile_1_year
field_percentile_dict[field]['Yearly'] = {}
field_percentile_dict[field]['Yearly']['Sum'] = yearly_field
field_percentile_dict[field]['Yearly']['ADV'] = yearly_adv
field_percentile_dict[field]['Yearly']['Percentile (Total)'] = yearly_percentile
field_percentile_dict[field]['Yearly']['Percentile (2-Year)'] = yearly_percentile_2_year

# Extract volume-based field up to daily level - aggregate volume by summing
    daily_field = product_orderfill.groupby(['Date'])[field].sum()
    # Attach month, quarter, and year to each row to allow groupby()
    yearmonth_col = pd.to_datetime(daily_field.index.strftime('%Y-%m'))
    yearquarter_col = \
        pd.to_datetime(daily_field.index.to_series()
                       .apply(lambda ts: f'{ts.year}-{month_to_quarter_shifter(ts.month):02}'))
    year_col = pd.to_datetime(daily_field.index.strftime('%Y'))
    daily_field_df = pd.DataFrame({field: daily_field, 'Month': yearmonth_col,
                                   'Quarter': yearquarter_col, 'Year': year_col})
    # No aggregation - daily
    # NOTE: at daily level, "volume" and "ADV" are conceptually the same
    daily_percentile = daily_field.rank(pct=True)    # Percentile over full history
    daily_percentile_1_year = daily_field.rolling(256).apply(lookback_rank, raw=False)    # Percentile rolling 1-year
    # Aggregate to monthly
    # NOTE: this was originally written with field='Volume', and I've generalized it to work
    #       with any field. "average daily volume" (ADV) is therefore too specific, but I won't
    #       change that for fear of confusion. so think of it as "average daily value" instead.
    # NOTE: ADV is what should be ranked - different months have different numbers of days, so sum doesn't work
    monthly_field = daily_field_df.groupby('Month')[field].sum()
    monthly_adv = daily_field_df.groupby('Month')[field].mean()
    monthly_percentile = monthly_adv.rank(pct=True)
    monthly_percentile_1_year = monthly_adv.rolling(12).apply(lookback_rank, raw=False)    # Percentile rolling 1-year
    # Aggregate to quarterly
    quarterly_field = daily_field_df.groupby('Quarter')[field].sum()
    quarterly_adv = daily_field_df.groupby('Quarter')[field].mean()
    quarterly_percentile = quarterly_adv.rank(pct=True)
    quarterly_percentile_1_year = quarterly_adv.rolling(4).apply(lookback_rank, raw=False)  # Percentile rolling 1-year
    # Aggregate to yearly
    yearly_field = daily_field_df.groupby('Year')[field].sum()
    yearly_adv = daily_field_df.groupby('Year')[field].mean()
    yearly_percentile = yearly_adv.rank(pct=True)
    yearly_percentile_2_year = yearly_adv.rolling(2).apply(lookback_rank, raw=False)  # Percentile rolling 2-year

    # Store
    field_percentile_dict[field] = {}
    field_percentile_dict[field]['Daily'] = {}
    field_percentile_dict[field]['Daily']['Sum'] = daily_field
    field_percentile_dict[field]['Daily']['ADV'] = daily_field
    field_percentile_dict[field]['Daily']['Percentile (Total)'] = daily_percentile
    field_percentile_dict[field]['Daily']['Percentile (1-Year)'] = daily_percentile_1_year
    field_percentile_dict[field]['Monthly'] = {}
    field_percentile_dict[field]['Monthly']['Sum'] = monthly_field
    field_percentile_dict[field]['Monthly']['ADV'] = monthly_adv
    field_percentile_dict[field]['Monthly']['Percentile (Total)'] = monthly_percentile
    field_percentile_dict[field]['Monthly']['Percentile (1-Year)'] = monthly_percentile_1_year
    field_percentile_dict[field]['Quarterly'] = {}
    field_percentile_dict[field]['Quarterly']['Sum'] = quarterly_field
    field_percentile_dict[field]['Quarterly']['ADV'] = quarterly_adv
    field_percentile_dict[field]['Quarterly']['Percentile (Total)'] = quarterly_percentile
    field_percentile_dict[field]['Quarterly']['Percentile (1-Year)'] = quarterly_percentile_1_year
    field_percentile_dict[field]['Yearly'] = {}
    field_percentile_dict[field]['Yearly']['Sum'] = yearly_field
    field_percentile_dict[field]['Yearly']['ADV'] = yearly_adv
    field_percentile_dict[field]['Yearly']['Percentile (Total)'] = yearly_percentile
    field_percentile_dict[field]['Yearly']['Percentile (2-Year)'] = yearly_percentile_2_year

####

# LQD, IHB1 (1st term futures prices)
lqd = pd.read_csv(DOWNLOADS_DIR + 'LQD_bbg_2021-01-26.csv',
                  parse_dates=['Date'], index_col='Date')
ihb1 = pd.read_csv(DOWNLOADS_DIR + 'IHB1_bbg_2021-01-26.csv',
                   parse_dates=['Date'], index_col='Date')

# Correlation
# hyg_ibxxibhy = (hyg.join(ibxxibhy, how='inner', rsuffix='_IBXXIBHY')
#                 .rename({'PX_LAST': 'HYG', 'PX_LAST_IBXXIBHY': 'IBXXIBHY'}, axis=1)
#                 .drop('PX_VOLUME', axis=1)
#                 .sort_index())
# hyg_ibxxibhy_change = hyg_ibxxibhy.pct_change()
# hyg_ibxxibhy_change = hyg_ibxxibhy_change.loc['2018-09-10':]
lqd_ihb1 = (lqd.join(ihb1, how='inner', rsuffix='_IHB1')
            .rename({'PX_LAST': 'LQD', 'PX_LAST_IHB1': 'IHB1'}, axis=1)
            .drop(['PX_VOLUME', 'PX_VOLUME_IHB1'], axis=1)
            .sort_index())
lqd_ihb1_change = lqd_ihb1.pct_change()

# Plot
fig, ax = plt.subplots(figsize=(19.2, 10.8))
ax.set_title('Correlation: LQD vs. Front Month IBIG Futures')
for n in range(1, 4):
    ax.plot(lqd_ihb1_change['LQD'].rolling(n*21, center=False).corr(lqd_ihb1_change['IHB1']),
            label=f'{n}-Month Rolling Correlation')
overall_corr = lqd_ihb1_change.corr().iloc[1,0]
ax.axhline(overall_corr, label=f'Overall Correlation ({overall_corr*100:.1f}%)',
           color='k', linestyle='--')
ax.legend()


product = 'VX'
new_accounts_data = pd.read_csv(DOWNLOADS_DIR + f'{USE_DATE}_{product}_new_accounts.csv',
                                parse_dates=['Trade Date'])
daily_volume = new_accounts_data.groupby('Trade Date')['Size'].sum()/2
daily_volume.name = 'Volume'

# Aggregate to monthly ADV
yearmonth_strs = daily_volumes.index.strftime('%Y-%m')  # String form needed for .loc[]
monthly_adv = pd.Series()
for yearmonth_str in yearmonth_strs:
    monthly_adv.loc[pd.Timestamp(yearmonth_str)] = daily_volumes.loc[yearmonth_str].mean()
monthly_adv_percentiles = monthly_adv.rank(pct=True)    # Percentile over full history
monthly_adv_rolling_percentiles = \
    monthly_adv.rolling(12).apply(lambda x_ser: x_ser.rank(pct=True)[-1], raw=False)    # Percentile rolling 1-year
monthly_adv_df = pd.DataFrame({'ADV': monthly_adv, 'Percentile (Since 2018-03)': monthly_adv_percentiles,
                               'Percentile (1 Year)': monthly_adv_rolling_percentiles})

# IBIG
# 1) Load
ibig = (pd.read_csv(DOWNLOADS_DIR + f'IBIG_Daily_Volume_data_{USE_DATE}.csv',
                    parse_dates=['Month, Day, Year of Trading Dt'])
        .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
ibig_new_accounts = evaluate_new_accounts(vxm)
vxm_new_accounts.to_csv(DOWNLOADS_DIR + f'vxm_new_accounts_{USE_DATE}.csv')     # Export!
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
vxm_new_accounts_CTI = vxm_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
vxm_new_accounts_cumsum = vxm_new_accounts_CTI.cumsum()

# IBIG
# 1) Load
ibig = (pd.read_csv(DOWNLOADS_DIR + 'IBIG_Daily_Volume_data_2021-03-02.csv',
                    parse_dates=['Month, Day, Year of Trading Dt'])
        .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
ibig_new_accounts = evaluate_new_accounts(ibig)
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
ibig_new_accounts_CTI = ibig_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
ibig_new_accounts_cumsum = ibig_new_accounts_CTI.cumsum()
# 4) Export
ibig_new_accounts.to_csv(DOWNLOADS_DIR + 'ibig_new_accounts_2021-03-02.csv')

# Try it on IBHY
# 1) Load
ibhy = (pd.read_csv(DOWNLOADS_DIR + 'IBHY_Daily_Volume_data_2021-03-02.csv',
                    parse_dates=['Month, Day, Year of Trading Dt'])
        .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
ibhy_new_accounts = evaluate_new_accounts(ibhy)
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
ibhy_new_accounts_CTI = ibhy_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
ibhy_new_accounts_cumsum = ibhy_new_accounts_CTI.cumsum()
# 4) Export
ibhy_new_accounts.to_csv(DOWNLOADS_DIR + 'ibhy_new_accounts_2021-03-02.csv')

# Try it on big VIX
# 1) Load
vx = (pd.read_csv(DOWNLOADS_DIR + 'VX_Daily_Volume_data_2021-01-29.csv',
                  parse_dates=['Month, Day, Year of Trading Dt'])
      .rename({'Month, Day, Year of Trading Dt': 'Trade Date'}, axis=1))
# 2) Run function
vx_new_accounts = evaluate_new_accounts(vx)
# 3) Aggregate for Python visuals (step 2 already makes Excel-pivot-ready sheet)
vx_new_accounts_CTI = vx_new_accounts.groupby(['Trade Date', 'CTI'])['New Account'].sum().unstack()
vx_new_accounts_cumsum = vx_new_accounts_CTI.cumsum()


# def evaluate_volumes(data):
#     # Step 0: Check if any accounts switch CTI
#     ctiswitch_set = set(data[data['CTI'] == 4]['Account ']) & set(data[data['CTI'] != 4]['Account '])
#     if len(ctiswitch_set) != 0:
#         print(f"Accounts that switch CTI: {ctiswitch_set}")
#
#     # Step 1: Aggregate size by date, CTI, firm, account; each row will be unique
#     data_agg = data.groupby(['Trade Date', 'CTI', 'Name', 'Account '])['Size'].sum()
#
#     # Step 2: Generate "known" set of accounts for each day
#     data_days = data_agg.index.get_level_values('Trade Date').unique()
#     known_set_dict = {data_days[0]: set()}  # No known accounts on first day
#     for day, prev_day in zip(data_days[1:], data_days[:-1]):
#         known_set_dict[day] = \
#             (known_set_dict[prev_day]
#              | set(data_agg.loc[prev_day].index.get_level_values('Account ')))
#
#     # Step 3: Mark accounts each day that were not known at the beginning of the day
#     data_agg_reset = data_agg.reset_index('Account ')
#     for day in data_days:
#         # .values is great for doing stuff with no unique indexes
#         data_agg_reset.loc[day, 'New Account'] = \
#             (~data_agg_reset.loc[day]['Account '].isin(known_set_dict[day])).values
#     return data_agg_reset


# for col in ['Trade Time', 'Trade Date', 'Expire Date']:
#     modern_data[col] = pd.to_datetime(modern_data[col])     # Parse dates after concat to save time
# modern_data = modern_data.set_index('Trade Time').sort_index()

# axr.plot(trade_date_expiry_data['Size'], color='C1', label='Size')

# nine_three = vx_315.loc['2020-09-03']
# nine_three_exp1 = nine_three[nine_three['Expire Date'] == pd.Timestamp('2020-10-21')]

# Column sets
legacy_vwap_cols =

# Settlement for all of legacy is 3:15pm, so 30-second VWAP would be 3:14:30pm to 3:15:00pm
vwap_legacy = legacy_vix.between_time('15:14:30', '15:15')

# Settlement for modern requires incorporating mini-VIX on 2020-08-10 and switching to 2:59:30pm-3:00:00pm on 2020-10-26
vwap_modern_pre_mini = modern_vix.between_time('15:14:30', '15:15')


vwap_modern_mini =


# modern_data = pd.concat(modern_data_list).sort_values(['Trade Date', 'Trade Time', 'Expire Date'])[modern_volume_cols]
# legacy_data = pd.concat(legacy_data_list).sort_values(['Date', 'Entry Time', 'Expiry Date'])[legacy_volume_cols]

test = pd.read_csv(modern_data_dir+'20180320 VX Contra Trade Data.csv',
                   parse_dates=['Trade Date', 'Trade Time', 'Expire Date'])

# Fix Virtu switch
old_virtu = adv_change_data_indexed.loc['Virtu Financial BD LLC']
new_virtu = adv_change_data_indexed.loc['Virtu Americas LLC']
virtu_adv = old_virtu['ADV'].replace(np.NaN, 0) + new_virtu['ADV'].replace(np.NaN, 0)
virtu_diff = virtu_adv.diff()
adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'ADV'] = virtu_adv
adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'Difference in ADV'] = virtu_diff
adv_change_data_indexed = adv_change_data_indexed.drop(['Virtu Financial BD LLC', 'Virtu Americas LLC'])

adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'ADV'] = virtu_adv
adv_change_data_indexed.loc['Virtu Financial BD + Americas LLC', 'Difference in ADV'] = virtu_diff

{obj.maturity_date: self.maturity_date,
                                              obj.price: self.price,
                                              obj.trade_date: self.trade_date,
                                              obj.volume: self.volume,
                                              obj.open_interest: self.open_interest,
                                              obj.t_to_mat: self.t_to_mat,
                                              obj.rate: self.rate}

class CashInstr(Instrument):
    """
    Cash instrument, derived from financial instrument
    Defined by: 1) extracting a price time-series from Instrument through column names
    """
    def __init__(self, ts_df, **super_kwargs):
        """
        :param ts_df: time-series DataFrame with time and value
        :param super_kwargs: kwargs for passing to superclass
        """
        super().__init__(ts_df, **super_kwargs)

:param tradestats: (optional) relevant trade statistics, etc. to be included; default empty DataFrame

# # TYVIX Vol Regimes
# test = pd.read_csv('Y:/Research/Research1/Gaibo/S&P Webinar Figures/3 - predictive signaling raw data.csv', index_col='Date', parse_dates=True)
# _, axleft = plt.subplots()
# make_lineplot(tyvix.price(), 'TYVIX', ax=axleft)
# make_regime(tyvix.vol_regime()[2], 'High Vol Regime', 'grey', 'Date', 'Index Level', 'TYVIX Vol Regimes', ax=axleft)
# make_regime(tyvix.vol_regime()[1], 'Low Vol Regime', 'white', 'Date', 'Index Level', 'TYVIX Vol Regimes', ax=axleft)
# # axleft.autoscale(enable=True, axis='x', tight=True)
# axleft.legend(loc=2, fontsize=13)
# axleft.set_ylabel('Volatility Index (%)', fontsize=16)
# axright = axleft.twinx()
# axright.plot(test['10YR - 2YR Yield'], label='10yr - 2yr Yield', color='C1')
# axright.legend(loc=1, fontsize=13)
# axright.set_ylabel('% (Annualized)', fontsize=16)
# axright.set_xlim('2016-01-01', '2019-09-03')
# # axleft.autoscale(enable=True, axis='x', tight=True)
# # axright.set_ylim(20, 115)
# axleft.set_title('TYVIX Vol Regimes', fontsize=16)
# axleft.set_xlabel('Date', fontsize=16)

creditvix_data = pd.read_csv('data/creditvix_pc_bp_missing_4_months.csv',
                             index_col='Date', parse_dates=True)

tuvix = pd.read_csv('data/tuvix_cme.csv', index_col='Date', parse_dates=True)
fvvix = pd.read_csv('data/fvvix_cme.csv', index_col='Date', parse_dates=True)
tyvix_cme = pd.read_csv('data/tyvix_cme.csv', index_col='Date', parse_dates=True)
usvix = pd.read_csv('data/usvix_cme.csv', index_col='Date', parse_dates=True)

ten_two_list_low = get_regime_data_list(tyvix_lows, test['10YR - 2YR Yield'])
combined_ten_two_low = combine_data_list(map(lambda df: df.pct_change(), ten_two_list_low))
ten_two_list_high = get_regime_data_list(tyvix_highs, test['10YR - 2YR Yield'])
combined_ten_two_high = combine_data_list(map(lambda df: df.pct_change(), ten_two_list_high))

# regime_10_2 = pd.DataFrame({'regime': tyvix_hl, '10-2': test['10YR - 2YR Yield']}).dropna()
# # Lows
# low_acc = pd.Series()
# for interval in tyvix_lows:
#     changes = regime_10_2['10-2'].loc[interval[0]:interval[1]].diff().dropna()
#     low_acc = low_acc.append(changes)
# low_acc.mean()
# # Highs
# high_acc = pd.Series()
# for interval in tyvix_highs:
#     changes = regime_10_2['10-2'].loc[interval[0]:interval[1]].diff().dropna()
#     high_acc = high_acc.append(changes)
# high_acc.mean()

cdx_ig = Index(bbg_data['IBOXUMAE CBBT Curncy', 'PX_LAST'], 'CDX NA IG')
itraxx_ie = Index(bbg_data['ITRXEBE CBBT Curncy', 'PX_LAST'], 'iTraxx EU Main')
itraxx_xo = Index(bbg_data['ITRXEXE CBBT Curncy', 'PX_LAST'], 'iTraxx EU Xover')
itraxx_fs = Index(bbg_data['ITRXESE CBBT Curncy', 'PX_LAST'], 'iTraxx EU SenFin')

scaled_cdx_ig = cdx_ig.loc[roll_dates_df['CDX IG'].iloc[-1]:]   # Start with just the current series

srvix_undl_rv = (srvix.underlying.price().rolling(252).apply(
                     lambda p_252: (p_252.diff()**2).mean()**0.5 * 252**0.5, raw=False)) * 100
srvix_undl_rv = srvix_undl_rv.shift(-252)

full_data = pd.read_csv('../data/price_index_data.csv', index_col='Date', parse_dates=True)
eurostoxx_data = pd.read_csv('../data/sx5e_data.csv', index_col='Date', parse_dates=True)
sptr_data = pd.read_csv('../data/sptr_vix_data.csv', index_col='Date', parse_dates=True)
agg_data = pd.read_csv('../data/agg_data.csv', index_col='Date', parse_dates=True)

import sys
sys.path.insert(0, '/Users/gaibo.zhang/PycharmProjects/analysis_package/utility/')

[truncd_spx, truncd_ty1] = share_dateindex([spx.price(), ty1.price()])
make_lineplot([truncd_spx/truncd_spx[0], truncd_ty1/truncd_ty1[0]],
              ['SPX cumulative return', 'TY1 cumulative return'],
              ['C0', 'C4'])

[truncd_spx, truncd_ty1] = share_dateindex([spx.price(), ty1.price()])
make_lineplot([truncd_spx/truncd_spx[0], truncd_ty1/truncd_ty1[0]],
              ['SPX cumulative return', 'TY1 cumulative return'])

[joined_x_data, joined_y_data] = share_dateindex([instr_x.price_return(),
                                                                  instr_y.price_return()])
                x = joined_x_data.values.reshape(-1, 1)
                y = joined_y_data.values
                model = LinearRegression(fit_intercept=False).fit(x, y)

color_dict = {frozenset({spx, vix}): 'C1', frozenset({spx, hyg}): 'C2', frozenset({spx, ief}): 'C3',
                  frozenset({spx, sx5e}): 'C4', frozenset({spx, agg}): 'C5',
                  frozenset({vix, hyg}): 'C1', frozenset({vix, ief}): 'C2', frozenset({vix, sx5e}): 'C3',
                  frozenset({vix, agg}): 'C4',
                  frozenset({hyg, ief}): 'C1', frozenset({hyg, sx5e}): 'C2', frozenset({hyg, agg}): 'C3',
                  frozenset({ief, sx5e}): 'C1', frozenset({ief, agg}): 'C2',
                  frozenset({sx5e, agg}): 'C1'}

calc_log_returns = lambda arr: np.log(arr[-1]/arr[0]) if len(arr)>1 else np.NaN

elif granularity == 'intraday':
if time_start != time_end:
    print("WARNING: only time_start parameter is used for intraday.")
intraday_day = truncd_levels.loc[time_start, time_start + ONE_DAY]
return intraday_day.resample(intraday_interval, label='right', closed='right').pad()
elif granularity == 'multiday':
# NOTE: still uncertain about how to do this
return truncd_levels.resample(multiday_interval).pad()


def log_returns(self, time_start=None, time_end=None,
                do_resample=False, resample_interval=pd.Timedelta(minutes=5)):
    """ Calculate log returns
    :param time_start: start of time-series to use
    :param time_end: end of time-series to use
    :param do_resample: set True to resample to the granularity of <resample_interval>
    :param resample_interval: only relevant if resampling
    :return: pd.Series with 'time' and 'value'
    """
    if time_start is None:
        time_start = self.levels.first_valid_index()
    if time_end is None:
        time_end = self.levels.last_valid_index()
    truncd_levels = self.levels.truncate(time_start, time_end)
    if not self.is_intraday or not do_resample:
        # Data is already regular (i.e. daily or fixed-interval intraday) -
        # avoid messing with business days, etc.
        return np.log(truncd_levels).diff()
    else:
        # Data is intraday and irregular - resample to get fixed intervals
        resampled_truncd_levels = \
            truncd_levels.resample(resample_interval,
                                   label='right', closed='right').pad()
        return np.log(resampled_truncd_levels).diff()

table = pd.concat([vix_prices.describe().iloc[0:1].astype(int),
                       vix_prices.describe().iloc[1:],
                       vix_pct_changes.describe().iloc[1:]*100])

def make_scatterplot(x_data, y_data, xlabel=None, ylabel=None, title=None, ax=None):
    # Prepare Figure and Axes
    if ax is None:
        fig, ax = plt.subplots()
    else:
        fig = None
    # Plot
    x_data.to_frame(x_data.name).join(y_data.to_frame(y_data.name))
    ax.scatter(x_data, y_data)
    ax.grid()
    ax.plot([0, 1], [0, x_data.corr(y_data)], 'k')
    # Set labels
    if xlabel:
        ax.set_xlabel(xlabel)
    if ylabel:
        ax.set_ylabel(ylabel)
    if title:
        ax.set_title(title)
    return fig, ax

def make_scatterplot(x_instr, y_instr, ax=None):
    # Prepare Figure and Axes
    if ax is None:
        fig, ax = plt.subplots()
    else:
        fig = None
    # Plot
    x_returns = x_instr.price_return(logarithmic=False)
    y_returns = y_instr.price_return(logarithmic=False)
    joined_data = x_returns.to_frame(x_instr.name).join(y_returns.to_frame(y_instr.name), how='inner')
    ax.scatter(joined_data[x_instr.name], joined_data[y_instr.name])
    ax.grid()
    corr_x_y = x_returns.corr(y_returns)
    x_min = x_returns.min()
    x_max = x_returns.max()
    ax.plot([x_min, x_max], [x_min*corr_x_y, x_max*corr_x_y], 'k')
    # Set labels
    ax.set_xlabel("{} % Change".format(x_instr.name))
    ax.set_ylabel("{} % Change".format(y_instr.name))
    ax.set_title("Daily Percent Change: {} vs {}".format(x_instr.name, y_instr.name))
    return fig, ax

ax.set_xlabel("{} % Change".format(x_instr.name))
    ax.set_ylabel("{} % Change".format(y_instr.name))
    ax.set_title("Daily Percent Change: {} vs {}".format(x_instr.name, y_instr.name))

joined_data = x_data.to_frame('x').join(y_data.to_frame('y'), how='inner').dropna()

# Execute organized commands to create desired analyses
