# Completely inexplicably - the above for-loop works but the latter does not
# I'm 99% sure it's literally a race condition - the for-loop is slow enough to let
# the labels generate in the chart, whereas the bottom is so fast that the labels do not exist yet.
# You can check by running rest of graphing code and then doing axs[1].get_xticklabels(), vs.
# running rest of code with commented line attached and doing axs[1].get_xticklabels()
# The latter will show empty label text!
# Hopefully matplotlib fixes this bug. 2022-03-09.
for label in axs[1].get_xticklabels(which='major'):
    label.set(rotation=30, ha='right', rotation_mode='anchor')
# axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=30, ha='right', rotation_mode='anchor')

df = pd.DataFrame(np.random.randn(5, 10))

git remote add new_cboe https://stash.cboe.com/scm/mas/gaibo_modules.git
git push -u new_cboe develop
git branch --set-upstream-to new_cboe/master
git remote rename cboe deprecated_cboe

git config http.sslVerify false
conda config --set ssl_verify false

pd.DataFrame(item for item in s)

settle_data_df = settle_data_trim.pivot(index=INDEX_COLS, columns='Measure Names', values='Measure Values')
# settle_data_df = settle_data_trim.pivot_table(index=INDEX_COLS, columns='Measure Names', values='Measure Values')
# settle_data_df = settle_data_trim.set_index(INDEX_COLS + ['Measure Names']).squeeze().unstack()

# create_percentile_ranks additional commands
from functools import reduce
adv_sum = reduce(lambda x, y: x.add(y, fill_value=0), [pd.DataFrame(product_dict[prod]['Volume']['Monthly']).fillna(0) for prod in PRODUCTS])
days_in_months = adv_sum.loc['2021']['Sum'] / adv_sum.loc['2021']['ADV']
ytd_adv = adv_sum.loc['2021']['Sum'].sum() / days_in_months.sum()

# To reload a function from a module
# 1) If you imported without "import <module>", import module now
import options_data_tools
from options_data_tools import add_rate     # May be able to skip this line? Test next time
# 2) Now that module is imported, reload it and overwrite function
import importlib
importlib.reload(options_data_tools)
from options_data_tools import add_rate

# Git go back to state just before commit, i.e. previous commit but with changes in staging
git reset --soft HEAD^  # HEAD^ is the previous commit

# Reload after editing matplotlib style sheet!
plt.style.reload_library()
plt.style.use('cboe-fivethirtyeight')

# iboxx_oi_file_name = 'Current_Open_Interest_and_Volume_by_Futures_Root_F_data.csv'  # [CONFIGURE]
# ibhy_oi_data = pd.read_csv(DOWNLOADS_DIR+iboxx_oi_file_name, index_col='Trading Date', parse_dates=True).sort_index()
# ibhy_oi = ibhy_oi_data.groupby('Trading Date')['Current Open Interest'].sum()
# ibhy_volume_breakdown['Open Interest'] = ibhy_oi


ax.autoscale(enable=True, axis='x', tight=True)

##############################################################################

cds_index_dict = {'CDX NA IG': bbg_data['IBOXUMAE CBBT Curncy', 'PX_LAST'].dropna(),
                  'CDX NA HY': bbg_data['IBOXHYSE CBBT Curncy', 'PX_LAST'].dropna(),
                  'iTraxx EU Main': bbg_data['ITRXEBE CBBT Curncy', 'PX_LAST'].dropna(),
                  'iTraxx EU Xover': bbg_data['ITRXEXE CBBT Curncy', 'PX_LAST'].dropna(),
                  'iTraxx EU SenFin': bbg_data['ITRXESE CBBT Curncy', 'PX_LAST'].dropna()}

roll_dates_loc = 'P:/PrdDevSharedDB/BBG Pull Scripts/credit_roll_dates.csv'
roll_vals_loc = 'P:/PrdDevSharedDB/BBG Pull Scripts/credit_roll_prev_vals.csv'
roll_dates_df = pd.read_csv(roll_dates_loc, index_col='Unnamed: 0')
roll_vals_df = pd.read_csv(roll_vals_loc, index_col='Unnamed: 0')

names_list = ['CDX NA IG', 'CDX NA HY', 'iTraxx EU Main', 'iTraxx EU Xover', 'iTraxx EU SenFin']

scaled_cds_index_dict = {}
for name in names_list:
    cds_index = cds_index_dict[name]
    roll_dates_ser = roll_dates_df[name].dropna()   # Make sure no NaNs are involved
    current_roll_date = roll_dates_ser.iloc[-1]
    scaled_cds_index = cds_index.loc[current_roll_date:]  # Start with just the current series
    roll_vals_ser = roll_vals_df[name].dropna()
    series_countdown = roll_vals_ser.index[::-1]   # Start with recent series and go backwards
    for series in series_countdown:
        # If series is 32, then we scale 31 to 32's magnitude
        roll_date = roll_dates_ser.loc[series]
        prev_roll_date = roll_dates_ser.loc[series-1]
        try:
            new_val = scaled_cds_index.loc[roll_date]   # Get newly scaled roll date value
        except KeyError:
            continue
        old_val = roll_vals_ser.loc[series]
        new_old_ratio = new_val / old_val
        last_series = cds_index.loc[prev_roll_date:roll_date].iloc[:-1]     # Exclude day of roll
        scaled_last_series = last_series * new_old_ratio
        scaled_cds_index = pd.concat([scaled_last_series, scaled_cds_index])
    scaled_cds_index_dict[name] = scaled_cds_index

cds_index_df = pd.DataFrame.from_dict(cds_index_dict)
scaled_cds_index_df = pd.DataFrame.from_dict(scaled_cds_index_dict)

cds_index_df.plot()
scaled_cds_index_df.plot()

no_cdx_na_hy = scaled_cds_index_df.drop('CDX NA HY', axis=1)
no_cdx_na_hy.plot(color=['C0','C2','C3','C4'])
no_cdx_na_hy.to_csv('scaled_cds_indexes.csv')

##############################################################################

pd.set_option('display.max_rows', 30)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

import model.data_structures
import utility.graph_utilities
import importlib
importlib.reload(model.data_structures)
importlib.reload(utility.graph_utilities)

month_ret_ief = np.log(ief.price()).diff(21).dropna()
month_ret_vix = np.log(vix.price()).diff(21).dropna()
month_ret_tyvix = np.log(tyvix.price()).diff(21).dropna()
month_ret_spx = np.log(spx.price()).diff(21).dropna()
make_scatterplot(month_ret_vix, month_ret_ief)
make_scatterplot(month_ret_tyvix, month_ret_ief)
make_scatterplot(month_ret_vix, month_ret_spx)


spx_intraday_trade = pd.read_csv('data/spx_intraday_trade.csv')
combined_time = (spx_intraday_trade['TRADE_TIME'] +
                 ':' +
                 spx_intraday_trade['TRADE_TIME_THOU'].apply('{:0>3}'.format))
trade_time = pd.to_datetime(combined_time, format='%d%b%Y:%H:%M:%S:%f')


intraday_2015 = pd.read_csv('data/VIX_TYVIX_2015.csv')
intraday_2015['TRADE_DATE'] = pd.to_datetime(intraday_2015['TRADE_DATE'], format='%d%b%Y %H:%M:%S')
intraday_2015 = intraday_2015.set_index('TRADE_DATE')
intraday_2016 = pd.read_csv('data/VIX_TYVIX_2016.csv')
intraday_2016['TRADE_DATE'] = pd.to_datetime(intraday_2016['TRADE_DATE'], format='%d%b%Y %H:%M:%S')
intraday_2016 = intraday_2016.set_index('TRADE_DATE')
intraday = pd.concat([intraday_2015, intraday_2016]).sort_index()
intraday_VIX = intraday[intraday['UNDLY_INST_SYM']=='VIX'].drop('UNDLY_INST_SYM', axis=1)
intraday_TYVIX = intraday[intraday['UNDLY_INST_SYM']=='TYVIX'].drop('UNDLY_INST_SYM', axis=1)

intraday_vix_data = pd.read_csv('data/vix_intraday.csv', index_col='TRADE_DATE', parse_dates=True)
intraday_tyvix_data = pd.read_csv('data/tyvix_intraday.csv', index_col='TRADE_DATE', parse_dates=True)

vix_i = VolatilityIndex(intraday_vix_data, spx, 'VIX', vix_data.drop('VIX Close', axis=1))
tyvix_i = VolatilityIndex(intraday_tyvix_data, ty1, 'TYVIX', tyvix_data.drop('Close', axis=1))
