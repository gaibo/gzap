For the record, Bitbucket SSH format is EITHER:
git@bitbucket.org:<workspace_ID>/<repo_name>.git
or
ssh://git@bitbucket.org/<workspace_ID>/<repo_name>.git


# Restore submodule function
Manually update url in .gitmodules (SSH url works fine, but should be careful to leave out username??).
git submodule sync --recursive
git submodule update --recursive
Then go in and do "git checkout master" so it's tracking and not a detached head.
But in the future should probably do initial clone with:
git submodule update --init --recursive
To update to latest tips of remote branches:
git submodule update --recursive --remote


# How SSH for Bitbucket works
Install OpenSSH and run ssh-agent:
Do "ps -a | grep ssh-agent" in Bash terminal to check if SSH is running, do "eval $(ssh-agent)" if it's not.
For the record, $() is a "command substitution" in Bash, so that command evals the result of ssh-agent command, which is:
SSH_AUTH_SOCK=/tmp/ssh-b3KLV3gaQDKT/agent.1760; export SSH_AUTH_SOCK;
SSH_AGENT_PID=1761; export SSH_AGENT_PID;
echo Agent pid 1761;
This is because SSH needs to know via environment variable the socket to reach the newly created agent I think.
To kill the created agent (the quotes are optional syntax to remember command substitution):
eval "$(ssh-agent -k)"

Use ssh-keygen to generate SSH key pair:
ssh-keygen -t ed25519 -b 4096 -C "gaibozhang@yahoo.com" -f bitbucket_gaibo
Apparently the password is optional - set one if you don't want someone with your device to make commits.
Keep in mind that you will be prompted for password every time you do git push/pull/fetch.
ssh-keygen outputs 2 files - private and public keys literally raw in directory.

Add key to SSH agent:
ssh-add ~/bitbucket_gaibo
"To ensure the correct SSH key is used when connecting to Bitbucket, update or create your SSH configuration file (~/.ssh/config)
with the following settings:
Host bitbucket.org
  AddKeysToAgent yes
  IdentityFile ~/.ssh/bitbucket_gaibo"
Honestly I don't understand that one.

Add public key to Bitbucket Cloud:
Security->SSH Keys->Add Key
Label key with physical machine it's attached to so you remember which to disable.
Paste public key text.

Check SSH authentication:
ssh -T git@bitbucket.org


# After Git 2.23, to checkout remote branch and work on it locally
git switch remote_branch_name
git switch -c remote_branch_name remote_name/remote_branch_name  # If multiple remotes
# Before Git 2.23
git checkout remote_branch_name
git checkout -b remote_branch_name remote_name/remote_branch_name   # If multiple remotes

eval $(ssh-agent)
Agent pid 9700
ssh-add ~/.ssh/<private_key_file> (probably /c/Users/gzhang/.ssh/id_rsa)

ssh -T git@bitbucket.org

# Completely inexplicably - the above for-loop works but the latter does not
# I'm 99% sure it's literally a race condition - the for-loop is slow enough to let
# the labels generate in the chart, whereas the bottom is so fast that the labels do not exist yet.
# You can check by running rest of graphing code and then doing axs[1].get_xticklabels(), vs.
# running rest of code with commented line attached and doing axs[1].get_xticklabels()
# The latter will show empty label text!
# Hopefully matplotlib fixes this bug. 2022-03-09.
for label in axs[1].get_xticklabels(which='major'):
    label.set(rotation=30, ha='right', rotation_mode='anchor')
# axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=30, ha='right', rotation_mode='anchor')

df = pd.DataFrame(np.random.randn(5, 10))

git remote add new_cboe https://stash.cboe.com/scm/mas/gaibo_modules.git
git push -u new_cboe develop
git branch --set-upstream-to new_cboe/master
git remote rename cboe deprecated_cboe

git config http.sslVerify false
conda config --set ssl_verify false

pd.DataFrame(item for item in s)

settle_data_df = settle_data_trim.pivot(index=INDEX_COLS, columns='Measure Names', values='Measure Values')
# settle_data_df = settle_data_trim.pivot_table(index=INDEX_COLS, columns='Measure Names', values='Measure Values')
# settle_data_df = settle_data_trim.set_index(INDEX_COLS + ['Measure Names']).squeeze().unstack()

# create_percentile_ranks additional commands
from functools import reduce
# adv_sum = reduce(lambda x, y: x.add(y, fill_value=0), [pd.DataFrame(product_dict[prod]['Volume']['Monthly']).fillna(0) for prod in PRODUCTS])
adv_sum = reduce(lambda x, y: x.add(y, fill_value=0), [pd.DataFrame(PRODUCT_DICT[prod]['Volume']['Monthly']).fillna(0) for prod in PRODUCT_DICT.keys()])
notional_sum = reduce(lambda x, y: x.add(y, fill_value=0), [pd.DataFrame(PRODUCT_DICT[prod]['Notional']['Monthly']).fillna(0) for prod in PRODUCT_DICT.keys()])
interest_rate_sum = reduce(lambda x, y: x.add(y, fill_value=0), [pd.DataFrame(PRODUCT_DICT[prod]['Notional']['Monthly']).fillna(0) for prod in ['AMW_Weekly', 'AMB1', 'AMB3', 'AMT1', 'AMT3']])
days_in_months = adv_sum.loc['2021']['Sum'] / adv_sum.loc['2021']['ADV']
ytd_adv = adv_sum.loc['2021']['Sum'].sum() / days_in_months.sum()

# To reload a function from a module
# 1) If you imported without "import <module>", import module now
import options_data_tools
from options_data_tools import add_rate     # May be able to skip this line? Test next time
# 2) Now that module is imported, reload it and overwrite function
import importlib
importlib.reload(options_data_tools)
from options_data_tools import add_rate

# Git go back to state just before commit, i.e. previous commit but with changes in staging
git reset --soft HEAD^  # HEAD^ is the previous commit

# Reload after editing matplotlib style sheet!
plt.style.reload_library()
plt.style.use('cboe-fivethirtyeight')

# iboxx_oi_file_name = 'Current_Open_Interest_and_Volume_by_Futures_Root_F_data.csv'  # [CONFIGURE]
# ibhy_oi_data = pd.read_csv(DOWNLOADS_DIR+iboxx_oi_file_name, index_col='Trading Date', parse_dates=True).sort_index()
# ibhy_oi = ibhy_oi_data.groupby('Trading Date')['Current Open Interest'].sum()
# ibhy_volume_breakdown['Open Interest'] = ibhy_oi


ax.autoscale(enable=True, axis='x', tight=True)

##############################################################################

cds_index_dict = {'CDX NA IG': bbg_data['IBOXUMAE CBBT Curncy', 'PX_LAST'].dropna(),
                  'CDX NA HY': bbg_data['IBOXHYSE CBBT Curncy', 'PX_LAST'].dropna(),
                  'iTraxx EU Main': bbg_data['ITRXEBE CBBT Curncy', 'PX_LAST'].dropna(),
                  'iTraxx EU Xover': bbg_data['ITRXEXE CBBT Curncy', 'PX_LAST'].dropna(),
                  'iTraxx EU SenFin': bbg_data['ITRXESE CBBT Curncy', 'PX_LAST'].dropna()}

roll_dates_loc = 'P:/PrdDevSharedDB/BBG Pull Scripts/credit_roll_dates.csv'
roll_vals_loc = 'P:/PrdDevSharedDB/BBG Pull Scripts/credit_roll_prev_vals.csv'
roll_dates_df = pd.read_csv(roll_dates_loc, index_col='Unnamed: 0')
roll_vals_df = pd.read_csv(roll_vals_loc, index_col='Unnamed: 0')

names_list = ['CDX NA IG', 'CDX NA HY', 'iTraxx EU Main', 'iTraxx EU Xover', 'iTraxx EU SenFin']

scaled_cds_index_dict = {}
for name in names_list:
    cds_index = cds_index_dict[name]
    roll_dates_ser = roll_dates_df[name].dropna()   # Make sure no NaNs are involved
    current_roll_date = roll_dates_ser.iloc[-1]
    scaled_cds_index = cds_index.loc[current_roll_date:]  # Start with just the current series
    roll_vals_ser = roll_vals_df[name].dropna()
    series_countdown = roll_vals_ser.index[::-1]   # Start with recent series and go backwards
    for series in series_countdown:
        # If series is 32, then we scale 31 to 32's magnitude
        roll_date = roll_dates_ser.loc[series]
        prev_roll_date = roll_dates_ser.loc[series-1]
        try:
            new_val = scaled_cds_index.loc[roll_date]   # Get newly scaled roll date value
        except KeyError:
            continue
        old_val = roll_vals_ser.loc[series]
        new_old_ratio = new_val / old_val
        last_series = cds_index.loc[prev_roll_date:roll_date].iloc[:-1]     # Exclude day of roll
        scaled_last_series = last_series * new_old_ratio
        scaled_cds_index = pd.concat([scaled_last_series, scaled_cds_index])
    scaled_cds_index_dict[name] = scaled_cds_index

cds_index_df = pd.DataFrame.from_dict(cds_index_dict)
scaled_cds_index_df = pd.DataFrame.from_dict(scaled_cds_index_dict)

cds_index_df.plot()
scaled_cds_index_df.plot()

no_cdx_na_hy = scaled_cds_index_df.drop('CDX NA HY', axis=1)
no_cdx_na_hy.plot(color=['C0','C2','C3','C4'])
no_cdx_na_hy.to_csv('scaled_cds_indexes.csv')

##############################################################################

pd.set_option('display.max_rows', 30)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

import model.data_structures
import utility.graph_utilities
import importlib
importlib.reload(model.data_structures)
importlib.reload(utility.graph_utilities)

month_ret_ief = np.log(ief.price()).diff(21).dropna()
month_ret_vix = np.log(vix.price()).diff(21).dropna()
month_ret_tyvix = np.log(tyvix.price()).diff(21).dropna()
month_ret_spx = np.log(spx.price()).diff(21).dropna()
make_scatterplot(month_ret_vix, month_ret_ief)
make_scatterplot(month_ret_tyvix, month_ret_ief)
make_scatterplot(month_ret_vix, month_ret_spx)


spx_intraday_trade = pd.read_csv('data/spx_intraday_trade.csv')
combined_time = (spx_intraday_trade['TRADE_TIME'] +
                 ':' +
                 spx_intraday_trade['TRADE_TIME_THOU'].apply('{:0>3}'.format))
trade_time = pd.to_datetime(combined_time, format='%d%b%Y:%H:%M:%S:%f')


intraday_2015 = pd.read_csv('data/VIX_TYVIX_2015.csv')
intraday_2015['TRADE_DATE'] = pd.to_datetime(intraday_2015['TRADE_DATE'], format='%d%b%Y %H:%M:%S')
intraday_2015 = intraday_2015.set_index('TRADE_DATE')
intraday_2016 = pd.read_csv('data/VIX_TYVIX_2016.csv')
intraday_2016['TRADE_DATE'] = pd.to_datetime(intraday_2016['TRADE_DATE'], format='%d%b%Y %H:%M:%S')
intraday_2016 = intraday_2016.set_index('TRADE_DATE')
intraday = pd.concat([intraday_2015, intraday_2016]).sort_index()
intraday_VIX = intraday[intraday['UNDLY_INST_SYM']=='VIX'].drop('UNDLY_INST_SYM', axis=1)
intraday_TYVIX = intraday[intraday['UNDLY_INST_SYM']=='TYVIX'].drop('UNDLY_INST_SYM', axis=1)

intraday_vix_data = pd.read_csv('data/vix_intraday.csv', index_col='TRADE_DATE', parse_dates=True)
intraday_tyvix_data = pd.read_csv('data/tyvix_intraday.csv', index_col='TRADE_DATE', parse_dates=True)

vix_i = VolatilityIndex(intraday_vix_data, spx, 'VIX', vix_data.drop('VIX Close', axis=1))
tyvix_i = VolatilityIndex(intraday_tyvix_data, ty1, 'TYVIX', tyvix_data.drop('Close', axis=1))
